//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-19659101
// Driver 352.21
// Based on LLVM 3.4svn
//

.version 4.1
.target sm_30, texmode_independent
.address_size 64

	// .globl	convolution_direct

.entry convolution_direct(
	.param .u64 .ptr .global .align 4 convolution_direct_param_0,
	.param .u64 .ptr .const .align 4 convolution_direct_param_1,
	.param .u64 .ptr .global .align 4 convolution_direct_param_2,
	.param .u32 convolution_direct_param_3,
	.param .u32 convolution_direct_param_4
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<113>;
	.reg .b32 	%r<51>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd9, [convolution_direct_param_0];
	ld.param.u64 	%rd10, [convolution_direct_param_1];
	ld.param.u64 	%rd11, [convolution_direct_param_2];
	ld.param.u32 	%r16, [convolution_direct_param_3];
	ld.param.u32 	%r17, [convolution_direct_param_4];
	mov.b32	%r18, %envreg6;
	mov.u32 	%r19, %ntid.x;
	mul.lo.s32 	%r1, %r19, %r18;
	mov.u32 	%r20, %ctaid.x;
	mov.b32	%r2, %envreg3;
	mad.lo.s32 	%r21, %r20, %r19, %r2;
	mov.u32 	%r22, %tid.x;
	add.s32 	%r3, %r21, %r22;
	mov.u32 	%r23, %ctaid.y;
	mov.u32 	%r4, %ntid.y;
	mov.b32	%r5, %envreg4;
	mad.lo.s32 	%r24, %r23, %r4, %r5;
	mov.u32 	%r25, %tid.y;
	add.s32 	%r6, %r24, %r25;
	mov.f32 	%f108, 0f00000000;
	mov.f32 	%f102, %f108;
	mov.f32 	%f101, %f108;
	mov.f32 	%f100, %f108;
	setp.lt.s32	%p1, %r17, 1;
	@%p1 bra 	BB0_21;

	add.s32 	%r27, %r17, -4;
	setp.lt.s32	%p2, %r27, 0;
	mov.f32 	%f60, 0f00000000;
	mov.f32 	%f102, %f60;
	mov.f32 	%f101, %f60;
	mov.f32 	%f100, %f60;
	mov.u32 	%r50, 0;
	@%p2 bra 	BB0_14;

	add.s32 	%r30, %r2, %r22;
	mad.lo.s32 	%r33, %r20, %r19, %r30;
	add.s32 	%r35, %r5, %r25;
	mad.lo.s32 	%r37, %r23, %r4, %r35;
	mad.lo.s32 	%r7, %r16, %r37, %r33;
	mov.f32 	%f112, 0f00000000;
	mov.f32 	%f102, %f112;
	mov.f32 	%f101, %f112;
	mov.f32 	%f100, %f112;
	mov.u32 	%r28, 0;
	mov.u32 	%r49, %r28;

BB0_3:
	mov.f32 	%f107, %f112;
	mov.f32 	%f110, %f107;
	mad.lo.s32 	%r39, %r16, %r49, %r7;
	mul.wide.s32 	%rd12, %r39, 4;
	add.s64 	%rd19, %rd9, %rd12;
	mul.lo.s32 	%r40, %r17, %r49;
	mul.wide.s32 	%rd13, %r40, 4;
	add.s64 	%rd18, %rd10, %rd13;
	mov.u32 	%r46, %r17;
	mov.u32 	%r48, %r28;

BB0_4:
	mov.u32 	%r10, %r48;
	mov.u64 	%rd4, %rd19;
	mov.u64 	%rd3, %rd18;
	mov.u32 	%r9, %r46;
	ld.const.f32 	%f65, [%rd3+12];
	ld.const.f32 	%f66, [%rd3+8];
	ld.const.f32 	%f67, [%rd3];
	ld.const.f32 	%f68, [%rd3+4];
	ld.global.f32 	%f69, [%rd4+12];
	ld.global.f32 	%f70, [%rd4+8];
	ld.global.f32 	%f71, [%rd4];
	ld.global.f32 	%f72, [%rd4+4];
	fma.rn.ftz.f32 	%f101, %f68, %f72, %f101;
	fma.rn.ftz.f32 	%f100, %f67, %f71, %f100;
	fma.rn.ftz.f32 	%f102, %f66, %f70, %f102;
	fma.rn.ftz.f32 	%f110, %f65, %f69, %f110;
	add.s64 	%rd19, %rd4, 16;
	add.s64 	%rd18, %rd3, 16;
	add.s32 	%r11, %r9, -4;
	add.s32 	%r12, %r10, 4;
	setp.le.s32	%p3, %r12, %r27;
	mov.u32 	%r46, %r11;
	mov.u32 	%r48, %r12;
	@%p3 bra 	BB0_4;

	setp.eq.s32	%p4, %r11, 1;
	@%p4 bra 	BB0_10;

	setp.eq.s32	%p5, %r11, 2;
	@%p5 bra 	BB0_9;
	bra.uni 	BB0_7;

BB0_9:
	ld.global.f32 	%f79, [%rd4+16];
	ld.const.f32 	%f80, [%rd3+16];
	fma.rn.ftz.f32 	%f100, %f80, %f79, %f100;
	ld.global.f32 	%f81, [%rd4+20];
	ld.const.f32 	%f82, [%rd3+20];
	fma.rn.ftz.f32 	%f101, %f82, %f81, %f101;
	bra.uni 	BB0_11;

BB0_10:
	ld.global.f32 	%f83, [%rd4+16];
	ld.const.f32 	%f84, [%rd3+16];
	fma.rn.ftz.f32 	%f100, %f84, %f83, %f100;

BB0_11:
	bra.uni 	BB0_12;

BB0_7:
	setp.ne.s32	%p6, %r11, 3;
	mov.f32 	%f111, %f110;
	@%p6 bra 	BB0_13;

	ld.global.f32 	%f73, [%rd4+16];
	ld.const.f32 	%f74, [%rd3+16];
	fma.rn.ftz.f32 	%f100, %f74, %f73, %f100;
	ld.global.f32 	%f75, [%rd4+20];
	ld.const.f32 	%f76, [%rd3+20];
	fma.rn.ftz.f32 	%f101, %f76, %f75, %f101;
	ld.global.f32 	%f77, [%rd4+24];
	ld.const.f32 	%f78, [%rd3+24];
	fma.rn.ftz.f32 	%f102, %f78, %f77, %f102;

BB0_12:
	mov.f32 	%f111, %f110;

BB0_13:
	mov.f32 	%f112, %f111;
	add.s32 	%r49, %r49, 1;
	setp.lt.s32	%p7, %r49, %r17;
	mov.f32 	%f108, %f112;
	@%p7 bra 	BB0_3;
	bra.uni 	BB0_21;

BB0_14:
	add.s32 	%r42, %r50, %r6;
	mul.lo.s32 	%r43, %r50, %r17;
	mul.wide.s32 	%rd14, %r43, 4;
	add.s64 	%rd7, %rd10, %rd14;
	mad.lo.s32 	%r44, %r42, %r16, %r3;
	mul.wide.s32 	%rd15, %r44, 4;
	add.s64 	%rd8, %rd9, %rd15;
	setp.eq.s32	%p8, %r17, 1;
	@%p8 bra 	BB0_19;

	setp.eq.s32	%p9, %r17, 2;
	@%p9 bra 	BB0_18;
	bra.uni 	BB0_16;

BB0_18:
	ld.const.f32 	%f91, [%rd7];
	ld.global.f32 	%f92, [%rd8];
	fma.rn.ftz.f32 	%f100, %f91, %f92, %f100;
	ld.global.f32 	%f93, [%rd8+4];
	ld.const.f32 	%f94, [%rd7+4];
	fma.rn.ftz.f32 	%f101, %f94, %f93, %f101;
	mov.f32 	%f40, %f60;
	mov.f32 	%f109, %f40;
	bra.uni 	BB0_20;

BB0_19:
	ld.const.f32 	%f95, [%rd7];
	ld.global.f32 	%f96, [%rd8];
	fma.rn.ftz.f32 	%f100, %f95, %f96, %f100;
	mov.f32 	%f44, %f60;
	mov.f32 	%f109, %f44;
	bra.uni 	BB0_20;

BB0_16:
	setp.ne.s32	%p10, %r17, 3;
	mov.f32 	%f109, %f60;
	@%p10 bra 	BB0_20;

	ld.const.f32 	%f85, [%rd7];
	ld.global.f32 	%f86, [%rd8];
	fma.rn.ftz.f32 	%f100, %f85, %f86, %f100;
	ld.global.f32 	%f87, [%rd8+4];
	ld.const.f32 	%f88, [%rd7+4];
	fma.rn.ftz.f32 	%f101, %f88, %f87, %f101;
	ld.global.f32 	%f89, [%rd8+8];
	ld.const.f32 	%f90, [%rd7+8];
	fma.rn.ftz.f32 	%f102, %f90, %f89, %f102;
	mov.f32 	%f36, %f60;
	mov.f32 	%f109, %f36;

BB0_20:
	mov.f32 	%f106, %f109;
	mov.f32 	%f108, %f106;
	add.s32 	%r50, %r50, 1;
	setp.lt.s32	%p11, %r50, %r17;
	@%p11 bra 	BB0_14;

BB0_21:
	mad.lo.s32 	%r45, %r1, %r6, %r3;
	add.ftz.f32 	%f97, %f100, %f101;
	add.ftz.f32 	%f98, %f102, %f97;
	add.ftz.f32 	%f99, %f108, %f98;
	mul.wide.s32 	%rd16, %r45, 4;
	add.s64 	%rd17, %rd11, %rd16;
	st.global.f32 	[%rd17], %f99;
	ret;
}

	// .globl	spinFact
.entry spinFact(
	.param .u64 .ptr .global .align 8 spinFact_param_0,
	.param .u32 spinFact_param_1
)
{
	.reg .f32 	%f<6>;
	.reg .b32 	%r<9>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [spinFact_param_0];
	ld.param.u32 	%r1, [spinFact_param_1];
	mov.b32	%r2, %envreg3;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r5, %r4, %r3, %r2;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r7, %r5, %r6;
	shl.b32 	%r8, %r7, 1;
	cvt.rn.f64.u32	%fd1, %r8;
	mul.f64 	%fd2, %fd1, 0d400921FB54442D18;
	cvt.rn.f32.s32	%f1, %r1;
	cvt.ftz.f64.f32	%fd3, %f1;
	div.rn.f64 	%fd4, %fd2, %fd3;
	cvt.rn.ftz.f32.f64	%f2, %fd4;
	add.f64 	%fd5, %fd4, 0d3FF921FB54442D18;
	cvt.rn.ftz.f32.f64	%f3, %fd5;
	cos.approx.f32 	%f4, %f2;
	cos.approx.f32 	%f5, %f3;
	mul.wide.u32 	%rd2, %r7, 8;
	add.s64 	%rd3, %rd1, %rd2;
	st.global.v2.f32 	[%rd3], {%f4, %f5};
	ret;
}

	// .globl	bitReverse
.entry bitReverse(
	.param .u64 .ptr .global .align 8 bitReverse_param_0,
	.param .u64 .ptr .global .align 8 bitReverse_param_1,
	.param .u32 bitReverse_param_2,
	.param .u32 bitReverse_param_3
)
{
	.reg .f32 	%f<5>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [bitReverse_param_0];
	ld.param.u64 	%rd2, [bitReverse_param_1];
	ld.param.u32 	%r1, [bitReverse_param_2];
	ld.param.u32 	%r2, [bitReverse_param_3];
	mov.b32	%r3, %envreg3;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r5, %r4, %r3;
	mov.u32 	%r7, %tid.x;
	add.s32 	%r8, %r6, %r7;
	mov.b32	%r9, %envreg4;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mad.lo.s32 	%r12, %r11, %r10, %r9;
	mov.u32 	%r13, %tid.y;
	add.s32 	%r14, %r12, %r13;
	and.b32  	%r15, %r8, 1431655765;
	shl.b32 	%r16, %r15, 1;
	and.b32  	%r17, %r8, -1431655766;
	shr.u32 	%r18, %r17, 1;
	or.b32  	%r19, %r16, %r18;
	and.b32  	%r20, %r19, 858993459;
	shl.b32 	%r21, %r20, 2;
	and.b32  	%r22, %r19, -858993460;
	shr.u32 	%r23, %r22, 2;
	or.b32  	%r24, %r21, %r23;
	and.b32  	%r25, %r24, 252645135;
	shl.b32 	%r26, %r25, 4;
	and.b32  	%r27, %r24, -252645136;
	shr.u32 	%r28, %r27, 4;
	or.b32  	%r29, %r26, %r28;
	and.b32  	%r30, %r29, 16711935;
	shl.b32 	%r31, %r30, 8;
	and.b32  	%r32, %r29, -16711936;
	shr.u32 	%r33, %r32, 8;
	or.b32  	%r34, %r31, %r33;
	{
	.reg .b32 %lhs;
	.reg .b32 %rhs;
	shl.b32 	%lhs, %r34, 16;
	shr.b32 	%rhs, %r34, 16;
	add.u32 	%r35, %lhs, %rhs;
	}
	neg.s32 	%r36, %r1;
	and.b32  	%r37, %r36, 31;
	shr.u32 	%r38, %r35, %r37;
	mul.lo.s32 	%r39, %r14, %r2;
	add.s32 	%r40, %r39, %r8;
	mul.wide.u32 	%rd3, %r40, 8;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	add.s32 	%r41, %r38, %r39;
	mul.wide.u32 	%rd5, %r41, 8;
	add.s64 	%rd6, %rd1, %rd5;
	st.global.v2.f32 	[%rd6], {%f1, %f2};
	ret;
}

	// .globl	norm
.entry norm(
	.param .u64 .ptr .global .align 8 norm_param_0,
	.param .u32 norm_param_1
)
{
	.reg .f32 	%f<8>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [norm_param_0];
	ld.param.u32 	%r1, [norm_param_1];
	mov.b32	%r2, %envreg3;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r5, %r4, %r3, %r2;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r7, %r5, %r6;
	mov.b32	%r8, %envreg4;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mad.lo.s32 	%r11, %r10, %r9, %r8;
	mov.u32 	%r12, %tid.y;
	add.s32 	%r13, %r11, %r12;
	mad.lo.s32 	%r14, %r13, %r1, %r7;
	mul.wide.u32 	%rd2, %r14, 8;
	add.s64 	%rd3, %rd1, %rd2;
	ld.global.v2.f32 	{%f1, %f2}, [%rd3];
	cvt.rn.f32.s32	%f5, %r1;
	div.approx.ftz.f32 	%f6, %f2, %f5;
	div.approx.ftz.f32 	%f7, %f1, %f5;
	st.global.v2.f32 	[%rd3], {%f7, %f6};
	ret;
}

	// .globl	butterfly
.entry butterfly(
	.param .u64 .ptr .global .align 8 butterfly_param_0,
	.param .u64 .ptr .global .align 8 butterfly_param_1,
	.param .u32 butterfly_param_2,
	.param .u32 butterfly_param_3,
	.param .u32 butterfly_param_4,
	.param .u32 butterfly_param_5
)
{
	.reg .f32 	%f<22>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [butterfly_param_0];
	ld.param.u64 	%rd2, [butterfly_param_1];
	ld.param.u32 	%r1, [butterfly_param_3];
	ld.param.u32 	%r2, [butterfly_param_4];
	mov.b32	%r3, %envreg3;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %ctaid.x;
	mad.lo.s32 	%r6, %r5, %r4, %r3;
	mov.u32 	%r7, %tid.x;
	add.s32 	%r8, %r6, %r7;
	mov.b32	%r9, %envreg4;
	mov.u32 	%r10, %ntid.y;
	mov.u32 	%r11, %ctaid.y;
	mad.lo.s32 	%r12, %r11, %r10, %r9;
	mov.u32 	%r13, %tid.y;
	add.s32 	%r14, %r12, %r13;
	add.s32 	%r15, %r2, 31;
	and.b32  	%r16, %r15, 31;
	mov.u32 	%r17, 1;
	shl.b32 	%r18, %r17, %r16;
	and.b32  	%r19, %r2, 31;
	shr.s32 	%r20, %r1, %r19;
	shr.u32 	%r21, %r8, %r16;
	shl.b32 	%r22, %r21, %r19;
	add.s32 	%r23, %r18, -1;
	and.b32  	%r24, %r8, %r23;
	add.s32 	%r25, %r22, %r24;
	mad.lo.s32 	%r26, %r14, %r1, %r25;
	add.s32 	%r27, %r26, %r18;
	mul.lo.s32 	%r28, %r24, %r20;
	mul.wide.s32 	%rd3, %r26, 8;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	mul.wide.s32 	%rd5, %r27, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.v2.f32 	{%f5, %f6}, [%rd6];
	mul.wide.s32 	%rd7, %r28, 8;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.v2.u32 	{%r29, %r30}, [%rd8];
	ld.param.u32 	%r32, [butterfly_param_5];
	xor.b32  	%r34, %r30, %r32;
	xor.b32  	%r35, %r34, -2147483648;
	mov.b32 	 %f9, %r35;
	mov.b32 	 %f10, %r29;
	xor.b32  	%r36, %r29, -2147483648;
	mov.b32 	 %f11, %r36;
	mov.b32 	 %f12, %r34;
	fma.rn.ftz.f32 	%f13, %f5, %f10, %f1;
	fma.rn.ftz.f32 	%f14, %f5, %f12, %f2;
	neg.ftz.f32 	%f15, %f5;
	fma.rn.ftz.f32 	%f16, %f15, %f10, %f1;
	fma.rn.ftz.f32 	%f17, %f15, %f12, %f2;
	fma.rn.ftz.f32 	%f18, %f6, %f10, %f14;
	fma.rn.ftz.f32 	%f19, %f6, %f9, %f13;
	st.global.v2.f32 	[%rd4], {%f19, %f18};
	fma.rn.ftz.f32 	%f20, %f6, %f11, %f17;
	fma.rn.ftz.f32 	%f21, %f6, %f12, %f16;
	st.global.v2.f32 	[%rd6], {%f21, %f20};
	ret;
}

	// .globl	transpose
.entry transpose(
	.param .u64 .ptr .global .align 8 transpose_param_0,
	.param .u64 .ptr .global .align 8 transpose_param_1,
	.param .u32 transpose_param_2
)
{
	.reg .f32 	%f<5>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [transpose_param_0];
	ld.param.u64 	%rd2, [transpose_param_1];
	ld.param.u32 	%r1, [transpose_param_2];
	mov.b32	%r2, %envreg3;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r5, %r4, %r3, %r2;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r7, %r5, %r6;
	mov.b32	%r8, %envreg4;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mad.lo.s32 	%r11, %r10, %r9, %r8;
	mov.u32 	%r12, %tid.y;
	add.s32 	%r13, %r11, %r12;
	mad.lo.s32 	%r14, %r13, %r1, %r7;
	mad.lo.s32 	%r15, %r7, %r1, %r13;
	mul.wide.u32 	%rd3, %r14, 8;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	mul.wide.u32 	%rd5, %r15, 8;
	add.s64 	%rd6, %rd1, %rd5;
	st.global.v2.f32 	[%rd6], {%f1, %f2};
	ret;
}

	// .globl	transposeMatrix
.entry transposeMatrix(
	.param .u64 .ptr .global .align 4 transposeMatrix_param_0,
	.param .u64 .ptr .global .align 4 transposeMatrix_param_1,
	.param .u32 transposeMatrix_param_2
)
{
	.reg .f32 	%f<2>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [transposeMatrix_param_0];
	ld.param.u64 	%rd2, [transposeMatrix_param_1];
	ld.param.u32 	%r1, [transposeMatrix_param_2];
	mov.b32	%r2, %envreg3;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r5, %r4, %r3, %r2;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r7, %r5, %r6;
	mov.b32	%r8, %envreg4;
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mad.lo.s32 	%r11, %r10, %r9, %r8;
	mov.u32 	%r12, %tid.y;
	add.s32 	%r13, %r11, %r12;
	mad.lo.s32 	%r14, %r13, %r1, %r7;
	mad.lo.s32 	%r15, %r7, %r1, %r13;
	mul.wide.u32 	%rd3, %r14, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.f32 	%f1, [%rd4];
	mul.wide.u32 	%rd5, %r15, 4;
	add.s64 	%rd6, %rd1, %rd5;
	st.global.f32 	[%rd6], %f1;
	ret;
}

	// .globl	multiply
.entry multiply(
	.param .u64 .ptr .global .align 8 multiply_param_0,
	.param .u64 .ptr .global .align 8 multiply_param_1
)
{
	.reg .f32 	%f<14>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [multiply_param_0];
	ld.param.u64 	%rd2, [multiply_param_1];
	mov.b32	%r1, %envreg3;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	add.s32 	%r6, %r4, %r5;
	mul.wide.u32 	%rd3, %r6, 8;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	add.s64 	%rd5, %rd1, %rd3;
	ld.global.v2.f32 	{%f4, %f5}, [%rd5];
	mul.ftz.f32 	%f9, %f2, %f5;
	neg.ftz.f32 	%f10, %f9;
	mul.ftz.f32 	%f11, %f2, %f4;
	fma.rn.ftz.f32 	%f12, %f1, %f5, %f11;
	fma.rn.ftz.f32 	%f13, %f1, %f4, %f10;
	st.global.v2.f32 	[%rd5], {%f13, %f12};
	ret;
}

	// .globl	mySeparableConvolution
.entry mySeparableConvolution(
	.param .u64 .ptr .global .align 4 mySeparableConvolution_param_0,
	.param .u64 .ptr .global .align 4 mySeparableConvolution_param_1,
	.param .u64 .ptr .const .align 4 mySeparableConvolution_param_2,
	.param .u32 mySeparableConvolution_param_3,
	.param .u32 mySeparableConvolution_param_4
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<64>;
	.reg .b32 	%r<38>;
	.reg .b64 	%rd<18>;


	ld.param.u64 	%rd8, [mySeparableConvolution_param_0];
	ld.param.u64 	%rd9, [mySeparableConvolution_param_1];
	ld.param.u64 	%rd10, [mySeparableConvolution_param_2];
	ld.param.u32 	%r14, [mySeparableConvolution_param_3];
	ld.param.u32 	%r15, [mySeparableConvolution_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r16, %ctaid.x;
	mov.b32	%r2, %envreg3;
	mad.lo.s32 	%r17, %r16, %r1, %r2;
	mov.u32 	%r18, %tid.x;
	add.s32 	%r3, %r17, %r18;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %ntid.y;
	mov.b32	%r6, %envreg4;
	mad.lo.s32 	%r19, %r4, %r5, %r6;
	mov.u32 	%r20, %tid.y;
	add.s32 	%r7, %r19, %r20;
	sub.s32 	%r21, %r14, %r15;
	setp.lt.s32	%p1, %r3, %r21;
	setp.lt.s32	%p2, %r7, %r21;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB8_13;
	bra.uni 	BB8_1;

BB8_1:
	mov.b32	%r23, %envreg6;
	mul.lo.s32 	%r8, %r1, %r23;
	add.s32 	%r9, %r15, -4;
	setp.lt.s32	%p4, %r9, 0;
	mov.f32 	%f63, 0f00000000;
	mov.f32 	%f62, %f63;
	mov.f32 	%f61, %f63;
	mov.f32 	%f60, %f63;
	mov.u32 	%r37, 0;
	@%p4 bra 	BB8_4;

	add.s32 	%r26, %r2, %r18;
	mad.lo.s32 	%r29, %r16, %r1, %r26;
	add.s32 	%r31, %r6, %r20;
	mad.lo.s32 	%r32, %r4, %r5, %r31;
	mad.lo.s32 	%r33, %r14, %r32, %r29;
	mul.wide.s32 	%rd11, %r33, 4;
	add.s64 	%rd17, %rd9, %rd11;
	mov.f32 	%f63, 0f00000000;
	mov.f32 	%f62, %f63;
	mov.f32 	%f61, %f63;
	mov.f32 	%f60, %f63;
	mov.u32 	%r37, 0;
	mov.u64 	%rd16, %rd10;

BB8_3:
	mov.u64 	%rd2, %rd16;
	ld.const.f32 	%f37, [%rd2+12];
	ld.const.f32 	%f38, [%rd2+8];
	ld.const.f32 	%f39, [%rd2];
	ld.const.f32 	%f40, [%rd2+4];
	ld.global.f32 	%f41, [%rd17+12];
	ld.global.f32 	%f42, [%rd17+8];
	ld.global.f32 	%f43, [%rd17];
	ld.global.f32 	%f44, [%rd17+4];
	fma.rn.ftz.f32 	%f61, %f40, %f44, %f61;
	fma.rn.ftz.f32 	%f60, %f39, %f43, %f60;
	fma.rn.ftz.f32 	%f62, %f38, %f42, %f62;
	fma.rn.ftz.f32 	%f63, %f37, %f41, %f63;
	add.s64 	%rd17, %rd17, 16;
	add.s64 	%rd5, %rd2, 16;
	add.s32 	%r37, %r37, 4;
	setp.le.s32	%p5, %r37, %r9;
	mov.u64 	%rd16, %rd5;
	@%p5 bra 	BB8_3;

BB8_4:
	mad.lo.s32 	%r34, %r7, %r14, %r3;
	sub.s32 	%r13, %r15, %r37;
	setp.eq.s32	%p6, %r13, 1;
	mul.wide.s32 	%rd12, %r37, 4;
	add.s64 	%rd6, %rd10, %rd12;
	add.s32 	%r35, %r37, %r34;
	mul.wide.s32 	%rd13, %r35, 4;
	add.s64 	%rd7, %rd9, %rd13;
	@%p6 bra 	BB8_9;
	bra.uni 	BB8_5;

BB8_9:
	ld.const.f32 	%f55, [%rd6];
	ld.global.f32 	%f56, [%rd7];
	fma.rn.ftz.f32 	%f60, %f55, %f56, %f60;
	bra.uni 	BB8_10;

BB8_5:
	setp.eq.s32	%p7, %r13, 2;
	@%p7 bra 	BB8_8;
	bra.uni 	BB8_6;

BB8_8:
	ld.const.f32 	%f51, [%rd6];
	ld.global.f32 	%f52, [%rd7];
	fma.rn.ftz.f32 	%f60, %f51, %f52, %f60;
	ld.global.f32 	%f53, [%rd7+4];
	ld.const.f32 	%f54, [%rd6+4];
	fma.rn.ftz.f32 	%f61, %f54, %f53, %f61;

BB8_10:

BB8_11:

BB8_12:
	mad.lo.s32 	%r36, %r8, %r7, %r3;
	add.ftz.f32 	%f57, %f60, %f61;
	add.ftz.f32 	%f58, %f62, %f57;
	add.ftz.f32 	%f59, %f63, %f58;
	mul.wide.s32 	%rd14, %r36, 4;
	add.s64 	%rd15, %rd8, %rd14;
	st.global.f32 	[%rd15], %f59;

BB8_13:
	ret;

BB8_6:
	setp.ne.s32	%p8, %r13, 3;
	@%p8 bra 	BB8_12;

	ld.const.f32 	%f45, [%rd6];
	ld.global.f32 	%f46, [%rd7];
	fma.rn.ftz.f32 	%f60, %f45, %f46, %f60;
	ld.global.f32 	%f47, [%rd7+4];
	ld.const.f32 	%f48, [%rd6+4];
	fma.rn.ftz.f32 	%f61, %f48, %f47, %f61;
	ld.global.f32 	%f49, [%rd7+8];
	ld.const.f32 	%f50, [%rd6+8];
	fma.rn.ftz.f32 	%f62, %f50, %f49, %f62;
	bra.uni 	BB8_11;
}


