//
// Generated by LLVM NVPTX Back-End
//

.version 3.2
.target sm_20, texmode_independent
.address_size 64

	// .globl	transpose_kernel
.global .align 4 .b8 program_scope_temp[1048576];
.shared .align 8 .b8 __clc_wg_shared_buffer[32];
.shared .align 8 .b8 __clc_wg_scan_shared_buffer[8720];
// .shared .align 8 .b8 __clc_wg_reduce_shared_buffer[8720];
                                        // @transpose_kernel
.entry transpose_kernel(
	.param .u64 .ptr .global .align 4 transpose_kernel_param_0,
	.param .u64 .ptr .shared .align 4 transpose_kernel_param_1,
	.param .u32 transpose_kernel_param_2,
	.param .u32 transpose_kernel_param_3,
	.param .u32 transpose_kernel_param_4
)
{
	.reg .s32 	%r<17>;
	.reg .s64 	%rd<10>;

// BB#0:                                // %entry
	ld.param.u64 	%rd1, [transpose_kernel_param_0];
	ld.param.u64 	%rd2, [transpose_kernel_param_1];
	mov.u32	%r1, %ctaid.x;
	ld.param.u32 	%r2, [transpose_kernel_param_2];
	mov.u32	%r3, %ntid.x;
	ld.param.u32 	%r4, [transpose_kernel_param_3];
	ld.param.u32 	%r5, [transpose_kernel_param_4];
	mov.u32	%r6, %tid.x;
	mad.lo.s32 	%r7, %r3, %r1, %r6;
	mov.u32	%r8, %ctaid.y;
	mov.u32	%r9, %ntid.y;
	mov.u32	%r10, %tid.y;
	mad.lo.s32 	%r11, %r9, %r8, %r10;
	mad.lo.s32 	%r12, %r11, %r2, %r7;
	mul.wide.u32 	%rd3, %r12, 4;
	mov.u64 	%rd4, program_scope_temp;
	add.s64 	%rd5, %rd4, %rd3;
	ld.global.u32 	%r13, [%rd5];
	mad.lo.s32 	%r14, %r10, %r5, %r6;
	mul.wide.u32 	%rd6, %r14, 4;
	add.s64 	%rd7, %rd2, %rd6;
	st.shared.u32 	[%rd7], %r13;
	bar.sync	0;
	mad.lo.s32 	%r15, %r7, %r4, %r11;
	ld.shared.u32 	%r16, [%rd7];
	mul.wide.u32 	%rd8, %r15, 4;
	add.s64 	%rd9, %rd1, %rd8;
	st.global.u32 	[%rd9], %r16;
	ret;
}

	// .globl	RecursiveGaussian_kernel
.entry RecursiveGaussian_kernel(
	.param .u64 .ptr .global .align 4 RecursiveGaussian_kernel_param_0,
	.param .u32 RecursiveGaussian_kernel_param_1,
	.param .u32 RecursiveGaussian_kernel_param_2,
	.param .align 4 .b8 RecursiveGaussian_kernel_param_3[48]
)                                       // @RecursiveGaussian_kernel
{
	.reg .pred 	%p<5>;
	.reg .s16 	%rs<21>;
	.reg .f32 	%f<143>;
	.reg .s32 	%r<22>;
	.reg .s64 	%rd<12>;

// BB#0:                                // %entry
	ld.param.u32 	%r12, [RecursiveGaussian_kernel_param_1];
	mov.u32	%r14, %ctaid.x;
	mov.u32	%r15, %ntid.x;
	mov.u32	%r16, %tid.x;
	mad.lo.s32 	%r1, %r15, %r14, %r16;
	setp.ge.u32	%p1, %r1, %r12;
	@%p1 bra 	LBB1_6;
// BB#1:                                // %for.cond.preheader
	ld.param.u32 	%r13, [RecursiveGaussian_kernel_param_2];
	setp.lt.s32	%p2, %r13, 1;
	@%p2 bra 	LBB1_6;
// BB#2:                                // %for.body.lr.ph
	ld.param.u64 	%rd1, [RecursiveGaussian_kernel_param_0];
	mov.b64	%rd2, RecursiveGaussian_kernel_param_3;
	mov.u64 	%rd3, %rd2;
	ld.param.f32 	%f1, [%rd3+16];
	ld.param.f32 	%f2, [%rd3+20];
	ld.param.f32 	%f3, [%rd3+24];
	ld.param.f32 	%f4, [%rd3+28];
	ld.param.f32 	%f5, [%rd3+32];
	ld.param.f32 	%f6, [%rd3+36];
	neg.f32 	%f15, %f1;
	neg.f32 	%f19, %f2;
	mov.f32 	%f115, 0f00000000;
	mov.u64 	%rd6, program_scope_temp;
	mov.u32 	%r18, %r1;
	mov.u32 	%r19, %r13;
	mov.f32 	%f116, %f115;
	mov.f32 	%f117, %f115;
	mov.f32 	%f118, %f115;
	mov.f32 	%f119, %f115;
	mov.f32 	%f120, %f115;
	mov.f32 	%f121, %f115;
	mov.f32 	%f122, %f115;
	mov.f32 	%f123, %f115;
	mov.f32 	%f124, %f115;
	mov.f32 	%f125, %f115;
	mov.f32 	%f126, %f115;
LBB1_4:                                 // %for.body
                                        // =>This Inner Loop Header: Depth=1
	mov.f32 	%f38, %f122;
	mov.f32 	%f37, %f121;
	mov.f32 	%f36, %f120;
	mov.f32 	%f35, %f119;
	mul.wide.s32 	%rd4, %r18, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [%rd5];
	cvt.rn.f32.u16	%f43, %rs1;
	cvt.rn.f32.u16	%f44, %rs2;
	cvt.rn.f32.u16	%f45, %rs3;
	cvt.rn.f32.u16	%f46, %rs4;
	mul.rn.f32 	%f79, %f115, %f4;
	mul.rn.f32 	%f80, %f116, %f4;
	mul.rn.f32 	%f81, %f117, %f4;
	mul.rn.f32 	%f82, %f118, %f4;
	fma.rn.f32 	%f83, %f3, %f46, %f82;
	fma.rn.f32 	%f84, %f3, %f45, %f81;
	fma.rn.f32 	%f85, %f3, %f44, %f80;
	fma.rn.f32 	%f86, %f3, %f43, %f79;
	fma.rn.f32 	%f87, %f15, %f35, %f86;
	fma.rn.f32 	%f88, %f15, %f36, %f85;
	fma.rn.f32 	%f89, %f15, %f37, %f84;
	fma.rn.f32 	%f90, %f15, %f38, %f83;
	fma.rn.f32 	%f122, %f19, %f126, %f90;
	fma.rn.f32 	%f121, %f19, %f125, %f89;
	fma.rn.f32 	%f120, %f19, %f124, %f88;
	fma.rn.f32 	%f119, %f19, %f123, %f87;
	cvt.rzi.u16.f32	%rs5, %f119;
	cvt.rzi.u16.f32	%rs6, %f120;
	cvt.rzi.u16.f32	%rs7, %f121;
	cvt.rzi.u16.f32	%rs8, %f122;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.v4.u8 	[%rd7], {%rs5, %rs6, %rs7, %rs8};
	add.s32 	%r19, %r19, -1;
	add.s32 	%r18, %r18, %r12;
	setp.eq.s32	%p3, %r19, 0;
	mov.f32 	%f115, %f43;
	mov.f32 	%f116, %f44;
	mov.f32 	%f117, %f45;
	mov.f32 	%f118, %f46;
	mov.f32 	%f123, %f35;
	mov.f32 	%f124, %f36;
	mov.f32 	%f125, %f37;
	mov.f32 	%f126, %f38;
	@%p3 bra 	LBB1_3;
	bra.uni 	LBB1_4;
LBB1_3:                                 // %for.body.44.lr.ph
	add.s32 	%r21, %r13, 1;
	add.s32 	%r17, %r13, -1;
	mad.lo.s32 	%r20, %r12, %r17, %r1;
	mov.f32 	%f127, 0f00000000;
	mov.f32 	%f128, %f127;
	mov.f32 	%f129, %f127;
	mov.f32 	%f130, %f127;
	mov.f32 	%f131, %f127;
	mov.f32 	%f132, %f127;
	mov.f32 	%f133, %f127;
	mov.f32 	%f134, %f127;
	mov.f32 	%f135, %f127;
	mov.f32 	%f136, %f127;
	mov.f32 	%f137, %f127;
	mov.f32 	%f138, %f127;
	mov.f32 	%f139, %f127;
	mov.f32 	%f140, %f127;
	mov.f32 	%f141, %f127;
	mov.f32 	%f142, %f127;
LBB1_5:                                 // %for.body.44
                                        // =>This Inner Loop Header: Depth=1
	mov.f32 	%f54, %f130;
	mov.f32 	%f53, %f129;
	mov.f32 	%f52, %f128;
	mov.f32 	%f51, %f127;
	mul.wide.s32 	%rd8, %r20, 4;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [%rd9];
	cvt.rn.f32.u16	%f127, %rs9;
	cvt.rn.f32.u16	%f128, %rs10;
	cvt.rn.f32.u16	%f129, %rs11;
	cvt.rn.f32.u16	%f130, %rs12;
	mul.rn.f32 	%f95, %f134, %f6;
	mul.rn.f32 	%f96, %f133, %f6;
	mul.rn.f32 	%f97, %f132, %f6;
	mul.rn.f32 	%f98, %f131, %f6;
	fma.rn.f32 	%f99, %f5, %f51, %f98;
	fma.rn.f32 	%f100, %f5, %f52, %f97;
	fma.rn.f32 	%f101, %f5, %f53, %f96;
	fma.rn.f32 	%f102, %f5, %f54, %f95;
	fma.rn.f32 	%f103, %f15, %f142, %f102;
	fma.rn.f32 	%f104, %f15, %f141, %f101;
	fma.rn.f32 	%f105, %f15, %f140, %f100;
	fma.rn.f32 	%f106, %f15, %f139, %f99;
	fma.rn.f32 	%f71, %f19, %f135, %f106;
	fma.rn.f32 	%f72, %f19, %f136, %f105;
	fma.rn.f32 	%f73, %f19, %f137, %f104;
	fma.rn.f32 	%f74, %f19, %f138, %f103;
	add.s64 	%rd11, %rd6, %rd8;
	ld.global.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [%rd11];
	cvt.rn.f32.u16	%f107, %rs13;
	cvt.rn.f32.u16	%f108, %rs14;
	cvt.rn.f32.u16	%f109, %rs15;
	cvt.rn.f32.u16	%f110, %rs16;
	add.rn.f32 	%f111, %f74, %f110;
	add.rn.f32 	%f112, %f73, %f109;
	add.rn.f32 	%f113, %f72, %f108;
	add.rn.f32 	%f114, %f71, %f107;
	cvt.rzi.u16.f32	%rs17, %f114;
	cvt.rzi.u16.f32	%rs18, %f113;
	cvt.rzi.u16.f32	%rs19, %f112;
	cvt.rzi.u16.f32	%rs20, %f111;
	st.global.v4.u8 	[%rd11], {%rs17, %rs18, %rs19, %rs20};
	add.s32 	%r21, %r21, -1;
	sub.s32 	%r20, %r20, %r12;
	setp.gt.s32	%p4, %r21, 1;
	mov.f32 	%f131, %f51;
	mov.f32 	%f132, %f52;
	mov.f32 	%f133, %f53;
	mov.f32 	%f134, %f54;
	mov.f32 	%f135, %f139;
	mov.f32 	%f136, %f140;
	mov.f32 	%f137, %f141;
	mov.f32 	%f138, %f142;
	mov.f32 	%f139, %f71;
	mov.f32 	%f140, %f72;
	mov.f32 	%f141, %f73;
	mov.f32 	%f142, %f74;
	@%p4 bra 	LBB1_5;
LBB1_6:                                 // %cleanup
	ret;
}


