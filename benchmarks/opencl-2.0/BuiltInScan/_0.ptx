//
// Generated by LLVM NVPTX Back-End
//

.version 3.2
.target sm_20, texmode_independent
.address_size 64

	// .globl	group_scan_kernel
.shared .align 8 .b8 __clc_wg_shared_buffer[32];
.shared .align 8 .b8 __clc_wg_scan_shared_buffer[8720];
.shared .align 8 .b8 __clc_wg_reduce_shared_buffer[8720];
                                        // @group_scan_kernel
.entry group_scan_kernel(
	.param .u64 .ptr .global .align 4 group_scan_kernel_param_0,
	.param .u64 .ptr .global .align 4 group_scan_kernel_param_1
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<10>;
	.reg .s32 	%r<6>;
	.reg .s64 	%rd<84>;

// BB#0:                                // %entry
	ld.param.u64 	%rd17, [group_scan_kernel_param_1];
	ld.param.u64 	%rd19, [group_scan_kernel_param_0];
	mov.u32	%r3, %ctaid.x;
	mov.u32	%r1, %ntid.x;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd20, %r1, %r3;
	mov.u32	%r2, %tid.x;
	cvt.s64.s32	%rd2, %r2;
	add.s64 	%rd21, %rd2, %rd20;
	shl.b64 	%rd22, %rd21, 32;
	shr.s64 	%rd23, %rd22, 30;
	add.s64 	%rd24, %rd19, %rd23;
	ld.global.f32 	%f1, [%rd24];
	shr.u64 	%rd25, %rd2, 4;
	shr.u64 	%rd26, %rd2, 8;
	add.s64 	%rd27, %rd26, %rd2;
	add.s64 	%rd28, %rd27, %rd25;
	shl.b64 	%rd29, %rd28, 2;
	mov.u64 	%rd30, __clc_wg_scan_shared_buffer;
	add.s64 	%rd4, %rd30, %rd29;
	st.shared.f32 	[%rd4], %f1;
	shr.u64 	%rd79, %rd1, 1;
	setp.eq.s64	%p1, %rd79, 0;
	@%p1 bra 	LBB0_1;
// BB#2:                                // %for.body.lr.ph.i
	shl.b64 	%rd32, %rd2, 1;
	or.b64  	%rd6, %rd32, 1;
	mov.u64 	%rd81, 1;
LBB0_3:                                 // %for.body.i
                                        // =>This Inner Loop Header: Depth=1
	bar.sync	0;
	setp.ge.u64	%p2, %rd2, %rd79;
	@%p2 bra 	LBB0_5;
// BB#4:                                // %if.then.i
                                        //   in Loop: Header=BB0_3 Depth=1
	mul.lo.s64 	%rd33, %rd81, %rd6;
	add.s64 	%rd34, %rd33, -1;
	add.s64 	%rd35, %rd33, %rd81;
	add.s64 	%rd36, %rd35, -1;
	shr.u64 	%rd37, %rd34, 4;
	shr.u64 	%rd38, %rd34, 8;
	shr.u64 	%rd39, %rd36, 4;
	shr.u64 	%rd40, %rd36, 8;
	add.s64 	%rd41, %rd38, %rd33;
	add.s64 	%rd42, %rd41, %rd37;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd45, %rd30, %rd43;
	ld.shared.f32 	%f2, [%rd45+-4];
	add.s64 	%rd46, %rd40, %rd35;
	add.s64 	%rd47, %rd46, %rd39;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd30, %rd48;
	ld.shared.f32 	%f3, [%rd49+-4];
	add.rn.f32 	%f4, %f2, %f3;
	st.shared.f32 	[%rd49+-4], %f4;
LBB0_5:                                 // %if.end.i
                                        //   in Loop: Header=BB0_3 Depth=1
	shl.b64 	%rd81, %rd81, 1;
	shr.u64 	%rd79, %rd79, 1;
	setp.ne.s64	%p3, %rd79, 0;
	@%p3 bra 	LBB0_3;
	bra.uni 	LBB0_6;
LBB0_1:
	mov.u64 	%rd81, 1;
LBB0_6:                                 // %for.end.i
	setp.ne.s32	%p4, %r2, 0;
	@%p4 bra 	LBB0_8;
// BB#7:                                // %if.then.25.i
	add.s64 	%rd50, %rd1, -1;
	shr.u64 	%rd51, %rd50, 4;
	shr.u64 	%rd52, %rd50, 8;
	add.s64 	%rd53, %rd52, %rd1;
	add.s64 	%rd54, %rd53, %rd51;
	shl.b64 	%rd55, %rd54, 2;
	add.s64 	%rd57, %rd30, %rd55;
	mov.u32 	%r4, 0;
	st.shared.u32 	[%rd57+-4], %r4;
LBB0_8:                                 // %for.cond.35.preheader.i
	cvt.s64.s32 	%rd3, %rd21;
	setp.lt.u32	%p5, %r1, 2;
	@%p5 bra 	LBB0_13;
// BB#9:                                // %for.body.37.lr.ph.i
	shl.b64 	%rd59, %rd2, 1;
	or.b64  	%rd12, %rd59, 1;
	mov.u64 	%rd83, 1;
LBB0_10:                                // %for.body.37.i
                                        // =>This Inner Loop Header: Depth=1
	shr.u64 	%rd81, %rd81, 1;
	bar.sync	0;
	setp.ge.u64	%p6, %rd2, %rd83;
	@%p6 bra 	LBB0_12;
// BB#11:                               // %if.then.40.i
                                        //   in Loop: Header=BB0_10 Depth=1
	mul.lo.s64 	%rd60, %rd81, %rd12;
	add.s64 	%rd61, %rd60, -1;
	add.s64 	%rd62, %rd60, %rd81;
	add.s64 	%rd63, %rd62, -1;
	shr.u64 	%rd64, %rd61, 4;
	shr.u64 	%rd65, %rd61, 8;
	shr.u64 	%rd66, %rd63, 4;
	shr.u64 	%rd67, %rd63, 8;
	add.s64 	%rd68, %rd65, %rd60;
	add.s64 	%rd69, %rd68, %rd64;
	shl.b64 	%rd70, %rd69, 2;
	add.s64 	%rd72, %rd30, %rd70;
	ld.shared.f32 	%f5, [%rd72+-4];
	add.s64 	%rd73, %rd67, %rd62;
	add.s64 	%rd74, %rd73, %rd66;
	shl.b64 	%rd75, %rd74, 2;
	add.s64 	%rd76, %rd30, %rd75;
	ld.shared.u32 	%r5, [%rd76+-4];
	st.shared.u32 	[%rd72+-4], %r5;
	ld.shared.f32 	%f6, [%rd76+-4];
	add.rn.f32 	%f7, %f5, %f6;
	st.shared.f32 	[%rd76+-4], %f7;
LBB0_12:                                // %for.inc.65.i
                                        //   in Loop: Header=BB0_10 Depth=1
	shl.b64 	%rd83, %rd83, 1;
	setp.lt.u64	%p7, %rd83, %rd1;
	@%p7 bra 	LBB0_10;
LBB0_13:                                // %_Z29work_group_scan_inclusive_addf.exit
    bar.sync    0;
	ld.shared.f32 	%f8, [%rd4];
	add.rn.f32 	%f9, %f1, %f8;
	bar.sync	0;
	shl.b64 	%rd77, %rd3, 2;
	add.s64 	%rd78, %rd17, %rd77;
	st.global.f32 	[%rd78], %f9;
	ret;
}

	// .globl	global_scan_kernel
.entry global_scan_kernel(
	.param .u64 .ptr .global .align 4 global_scan_kernel_param_0,
	.param .u32 global_scan_kernel_param_1
)                                       // @global_scan_kernel
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .s32 	%r<17>;
	.reg .s64 	%rd<6>;

// BB#0:                                // %entry
	ld.param.u64 	%rd1, [global_scan_kernel_param_0];
	mov.u32	%r1, %tid.x;
	ld.param.u32 	%r6, [global_scan_kernel_param_1];
	mov.u32	%r2, %ctaid.x;
	mov.u32	%r3, %ntid.x;
	and.b32  	%r7, %r6, 31;
	mov.u32 	%r8, 1;
	shl.b32 	%r4, %r8, %r7;
	shr.s32 	%r9, %r2, %r7;
	shl.b32 	%r10, %r4, 1;
	mad.lo.s32 	%r5, %r10, %r9, %r4;
	setp.eq.s32	%p1, %r1, 0;
	@%p1 bra 	LBB1_2;
// BB#1:                                // %if.end
	membar.gl;
	bar.sync	0;
	bra.uni 	LBB1_3;
LBB1_2:                                 // %if.then.i
	mad.lo.s32 	%r11, %r5, %r3, -1;
	mul.wide.u32 	%rd2, %r11, 4;
	add.s64 	%rd3, %rd1, %rd2;
	ld.global.u32 	%r12, [%rd3];
	membar.gl;
	bar.sync	0;
	st.shared.u32 	[__clc_wg_shared_buffer], %r12;
LBB1_3:                                 // %_Z20work_group_broadcastfm.exit
	bar.sync	0;
	ld.shared.f32 	%f1, [__clc_wg_shared_buffer];
	bar.sync	0;
	add.s32 	%r13, %r4, -1;
	and.b32  	%r14, %r2, %r13;
	add.s32 	%r15, %r5, %r14;
	mad.lo.s32 	%r16, %r15, %r3, %r1;
	mul.wide.u32 	%rd4, %r16, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f2, [%rd5];
	add.rn.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd5], %f3;
	ret;
}


