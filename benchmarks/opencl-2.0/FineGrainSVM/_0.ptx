//
// Generated by LLVM NVPTX Back-End
//

.version 3.2
.target sm_20
.address_size 64

	// .globl	fine_grain_ldstore
.weak .shared .align 8 .b8 __clc_wg_shared_buffer[32];
.weak .shared .align 8 .b8 __clc_wg_scan_shared_buffer[8720];
.weak .shared .align 8 .b8 __clc_wg_reduce_shared_buffer[8720];
                                        // @fine_grain_ldstore
.visible .entry fine_grain_ldstore(
	.param .u64 fine_grain_ldstore_param_0,
	.param .u64 fine_grain_ldstore_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .s32 	%r<10>;
	.reg .s64 	%rd<8>;

// BB#0:                                // %entry
	ld.param.u64 	%rd2, [fine_grain_ldstore_param_0];
	ld.param.u64 	%rd3, [fine_grain_ldstore_param_1];
	mov.u32	%r3, %ctaid.x;
	mov.u32	%r4, %ntid.x;
	mov.u32	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	cvt.s64.s32	%rd1, %r1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	cvt.u32.u64	%r2, %rd5;
LBB0_1:                                 // %while.cond
                                        // =>This Inner Loop Header: Depth=1
	atom.global.scacq.sys.ld.s32 	%r6, [%r2];
	setp.ne.s32	%p1, %r6, 99;
	@%p1 bra 	LBB0_1;
// BB#2:                                // %while.end
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd2, %rd6;
	ld.global.u32 	%r7, [%rd7];
	add.s32 	%r8, %r7, %r1;
	st.global.u32 	[%rd7], %r8;
	add.s32 	%r9, %r1, 100;
	atom.global.screl.sys.st.s32 	[%r2], %r9;
	ret;
}


